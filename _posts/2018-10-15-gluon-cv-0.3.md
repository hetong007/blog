---
title: "GluonCV 0.3: 超越经典"
author: 张帜 Amazon Applied Scientist
---

半年前我们开始了 GluonCV 项目，希望提供一个可靠的可以重复各个论文结果的深度学习计算机视觉库。过去几个月里小伙伴们挖掘了大量论文和实现中的隐藏细节，并对模型训练的各个细节进行了大量的实验。我们兴奋地发现，我们不仅仅能重复结果，而且能超越它们。

现在，我们高兴地宣布 GluonCV 的新的0.3版。在这个版本里我们加入了5个新算法和38个新训练好的模型。并且对28个0.2版的模型进行了大的改进。下面表格对比了其中的7个模型：

| Model               | Metric                | 0.2    | 0.3    | Reference                                                    |
| ------------------- | --------------------- | ------ | ------ | ------------------------------------------------------------ |
| [ResNet-50](https://gluon-cv.mxnet.io/model_zoo/classification.html#resnet)           | top-1 acc on ImageNet | 77.07% | **79.15%** | 75.3% ([Caffe impl](https://github.com/KaimingHe/deep-residual-networks)) |
| [ResNet-101](https://gluon-cv.mxnet.io/model_zoo/classification.html#resnet)           | top-1 acc on ImageNet | 78.81% | **80.51%** | 76.4% ([Caffe impl](https://github.com/KaimingHe/deep-residual-networks)) |
| [MobileNet 1.0](https://gluon-cv.mxnet.io/model_zoo/classification.html#mobilenet) | top-1 acc on ImageNet |  N/A | **73.28%** | 70.9% ([tensorflow impl)](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md) |
| [Faster-RCNN](https://gluon-cv.mxnet.io/model_zoo/detection.html#id37)         | mAP on COCO           | N/A    | **40.1%**  | 39.6% ([Detectron](https://github.com/facebookresearch/Detectron)) |
| [Yolo-v3](https://gluon-cv.mxnet.io/model_zoo/detection.html#id44)             | mAP on COCO           | N/A    | **37.0%**  | 33.0% ([paper](https://pjreddie.com/media/files/papers/YOLOv3.pdf)) |
| [DeepLab-v3](https://gluon-cv.mxnet.io/model_zoo/segmentation.html#semantic-segmentation)          | mIoU on VOC           | N/A    | **86.7%**  | 85.7% ([paper](https://arxiv.org/abs/1706.05587))            |
| [Mask-RCNN](https://gluon-cv.mxnet.io/model_zoo/segmentation.html#instance-segmentation)   | mask AP on COCO       | N/A    | **33.1%**  | 32.8% ([Detectron](https://github.com/facebookresearch/Detectron)) |



值得一提的是，我们大幅提升了 ResNet 系列模型精度。我们训练得到的 ResNet-50 比论文中 ResNet-152 的精度要高将近一个点（top-1 acc），但计算上便宜三倍。我们还将 YOLOv3 的精度（mAP）提升了4个点，它跟使用 ResNet-50 的 Faster RCNN 精度持平，但计算速度和内存效率均提升了10倍。

同时，我们对 Model Zoo 页面进行了改版。现在我们可以交互地比较各个模型的预测精度和性能。

此外，我们还改进了模型部署能力。所有模型都可以脱离 Python 运行（就是可以 hybridize ）。

下面我们具体介绍0.3版本的改进。

![](img/gluon-cv-0.3.png)

## 研究和刷分

GluonCV 绝对是刷榜实战利器，用默认的参数你可以轻松排上各种公开数据集的排行榜。同时模块化的设计让你能很快地实现各种新的想法，投稿截止的压力不再沉重。 0.3 还提供了一些准备专有数据集的教程，小伙伴们能以此为基础在新的数据上快速准备环境。在接下来的时间里，我们会继续补充各种教程，帮助大家更得心应手地使用 GluonCV。

猛击下图查看图像分类任务中各个预训练模型的准确率：

<div style="max-width: 800px;" >
{% include classification_throughputs.html %}
</div>

猛击下图查看目标检测任务中各个预训练模型的准确率：

<div style="max-width: 800px;" >
{% include detection_throughputs.html %}
</div>

## 0.3版本介绍

### 新模型：[YOLOv3](https://gluon-cv.mxnet.io/model_zoo/detection.html#ms-coco)

YOLOv3 是 YOLOv1 之后性能提升最明显的版本，我们在复现论文的基础上加入了一些改动，把 YOLOv3 在 COCO 上的 mAP 从论文中的 33% 刷到了 37% （分辨率 608 ），相当于达到 Faster-RCNN + ResNet50 的准确度的同时，速度至少快了 10 倍！

### 新模型： [Mask-RCNN](https://gluon-cv.mxnet.io/model_zoo/segmentation.html#instance-segmentation)

Mask-RCNN 是基于 Faster-RCNN 系列的多任务输出模型，在目标检测的基础上增加了对每个物体做语义分割预测的功能，我们提供了基于 ResNet-50 的预训练模型，得到了 38.3% 的目标检测 mAP 和 33.1% 的语义分割 mAP，效果略好于 Detectron 的等价模型。

### 新模型：[DeepLabV3](https://gluon-cv.mxnet.io/model_zoo/segmentation.html#semantic-segmentation)

DeepLabV3 是基于 FCN 的端对端模型，利用多尺度的上下文信息提高语义分割的精度，在 Pascal VOC 和 ADE20K 数据集上都有出色的表现，我们复现的模型在 VOC 数据集上超越原文精度，达到了 86.7 mIoU 的高精度表现。

### 性能优化：[Faster-RCNN](https://gluon-cv.mxnet.io/model_zoo/detection.html#ms-coco)

Faster-RCNN 优化的亮点是我们成功基于普通的 ResNet-101 ，在不加 FPN 的情况下刷到了 COCO 上 40.1% 的 mAP ，超过了 Detectron 里 ResNet-101 + FPN 的 39.6% 的 mAP 。基于 ResNet-50 的模型也达到了 37% 的高分，同样是市面上最好的开源 Faster-RCNN 实现，接下来版本中我们还会加入 FPN 的支持。关于刷分用到的黑科技，敬请期待我们的新论文。

### 性能优化：[图像识别](https://gluon-cv.mxnet.io/model_zoo/classification.html)

你可曾想过，在 ImageNet 上能让 ResNet-50 超过 79% ，让 ResNet-101 超过 80% 吗？我们最近通过微调模型结构与加入训练过程上的优化，在基本不增加计算量和模型复杂度的前提下把ResNet模型的精度提高到了目前最好的成绩。训练过程上的优化不只对 ResNet 起效，我们也将 MobileNet ， Darknet53 ， Inception V3 提高到了比原文更好的精度。同时，这些更好的模型在目标检测和语义分割等其他任务上也达到了更好的效果，证明了这样的结果并不是对 ImageNet 的过拟合。

详细信息请参考 GluonCV 的[Model Zoo](https://gluon-cv.mxnet.io/model_zoo/classification.html)页面，我们也会在新论文中讨论相关的细节，敬请期待。

### 新应用：[GAN](https://github.com/dmlc/gluon-cv/tree/master/scripts/gan/wgan)

0.3 版新增了热门的 GAN 的应用模型，[@husonchen](https://github.com/husonchen) 贡献了 WGAN 的模型和训练脚本，比如说下图就是用 GAN 直接生成的卧室图片。

![](img/gan-room-fake.png)

### 新应用：[行人重识别](https://github.com/dmlc/gluon-cv/tree/master/scripts/re-id/baseline)

行人重识别是安全领域相当重要的应用，在 0.3 版本中[@xiaolai-sqlai](https://github.com/xiaolai-sqlai)贡献了行人重识别训练模块，在 market1501 数据集上可以取得 93.1% 的最好结果。

### 性能优化：[语义分割](https://gluon-cv.mxnet.io/model_zoo/segmentation.html)

在语义分割方面，随着[跨卡的 SyncBatchNorm 加入 MXNet](https://zh.mxnet.io/blog/syncbn) ，我们提供了完整到基于 SyncBN 的训练代码，完美复现 SOTA 算法。另外我们还加入了 CityScapes 数据集，并且提供了 PSPNet 的预训练模型。 Pascal VOC 上的实验复现非常繁琐，为此我们提供了一个[详细的教程](https://gluon-cv.mxnet.io/build/examples_segmentation/voc_sota.html)，一步一步讲解如何复现实验。使用 GluonCV 不仅可以一键复现最好的结果，还可以了解详细过程中的原理。

### 新功能：[部署](https://gluon-cv.mxnet.io/build/examples_deployment/export_network.html)

有了训练好的模型不知道怎么部署？ GluonCV 也没有忘记这些小伙伴，我们提供了一键导出函数，以及用 C++ 预测的样例代码，让你的模型更快地上线实战。如果上述的预训练模型已经足以满足需求，你甚至可以跳过训练步骤直接投入应用。

## 下一步

- GluonCV 链接：[https://gluon-cv.mxnet.io/index.html](https://gluon-cv.mxnet.io/index.html)

- GluonCV GitHub： [https://github.com/dmlc/gluon-cv](https://github.com/dmlc/gluon-cv)

- 论坛讨论：[https://discuss.gluon.ai](https://discuss.gluon.ai/t/topic/8517)

喜欢这一版的话不要忘了[点赞收藏加星](https://github.com/dmlc/gluon-cv)！
